{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Linear Algebra\n",
    "Sources: \n",
    "* 3Blue1Brown. \"The Essence of Linear Algebra\" video series. [[src.]](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&ab_channel=3Blue1Brown)\n",
    "* Sam Levey. \"The Matrix Transpose: Visual Intuition\" video. [[src.]](https://www.youtube.com/watch?v=wjYpzkQoyD8&ab_channel=SamLevey)\n",
    "* Wikipedia. Linear Algebra. [[src.]](https://en.wikipedia.org/wiki/Linear_algebra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. General Description of Linear algebra\n",
    "Linear algebra is a branch of mathematics concerning:\n",
    "1. linear equation such as $a_1x_1 + \\dots + a_nx_n = b $\n",
    "2. linear maps such as $(x_1,\\dots ,x_n)\\mapsto a_1x_1 + \\dots + a_nx_n$\n",
    "3. the methematical representation in vector spaces through matrices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In \"Linear\" algebra, we only concerns with the transformation of spaces where, casually speaking:\n",
    "1. all lines in space remain lines i.e. grid lines remains straight and evenly spaced, and\n",
    "2. origin remains fixed in space. \n",
    "\n",
    "The implication of this constraint limits our scope and operations to those of linear functions, where $f(x+y)=f(x)+f(y)$ and $f(ax)=af(x)$. This means inputs are proportional with outputs and the functions are additive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The application of Linear Algebra\n",
    "\n",
    "Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis, a branch of mathematical analysis, may be viewed as the application of linear algebra to function spaces.\n",
    "\n",
    "Linear algebra is also used in most sciences and fields of engineering, because it allows modeling many natural phenomena, and computing efficiently with such models. For nonlinear systems, which cannot be modeled with linear algebra, it is often used for dealing with [first-order approximations](https://en.wikipedia.org/wiki/Order_of_approximation#First-order), using the fact that the differential of a multivariate function at a point is the linear map that best approximates the function near that point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>What is \"order of approximation\"?</strong></summary>\n",
    "\n",
    "Order of approximation refers to how \"accurate\" an approximation is. However, it is important to note that \"accuracy\" of approximation in this case does not refer to the absolute errors, but more precisely the polynomial degree of series expansion. If a function is analytic, the Taylor series centered at that point is the limit of a sequence of polynomials, each of which provides successively better approximations to the function around that point. [[Taylor Series vs Fourier Series]](https://www.reddit.com/r/askmath/comments/jqn35q/advantages_to_taylor_series_or_fourier_series/)\n",
    "\n",
    "<img src=\"./img/taylor_series.png\" alt=\"Taylor Series with varying approximation terms\" width=\"400px\" align=\"center\">\n",
    "\n",
    "In the case of a smooth function, the $n^{th}$-order approximation is a polynomial of degree n, which is obtained by truncating the Taylor series to this degree. The error usually varies within the interval. Thus, the terms (zeroth, first, second, etc.) used above do not directly give information about percent error or significant figures. \n",
    "\n",
    "The ordinal number used before the word \"*-order\" refers to the highest power in the series expansion used in the approximation. The choice of order of approximation depends on the research purpose. One may wish to simplify a known analytic expression to devise a new application or, on the contrary, try to fit a curve to data points. Higher order of approximation is not always more useful than the lower one. For example, if a quantity is constant within the whole interval, approximating it with a second-order Taylor series will not increase the accuracy.\n",
    "\n",
    "For example, \n",
    "* **zeroth-order** approximation refers to the first rough answer where we are only confident on the order of magnitude such as \"a few thousands\". $y\\sim f(x)=a$\n",
    "* This contrasts with **first-order** approximation where we may give at least one exact number such as \"$4\\times 10^3$\". First-order approximation of a function is a linear approximation (straight line with a slope): polynomial of degree 1. Informally, we may call first-order approximation an \"educated guess\". $y\\sim f(x)=a_1x+a_2$\n",
    "* **Second-order** approximation can then be considered a \"decent-quality\" answer. Second-order approximation of a function is a quadratic polynomial (geometrically, this is a parabola): polynomial of degree 2. $y\\sim f(x)=a_1x^2+a_2x+a_3$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>How to approximate a non-linear function?</strong></summary>\n",
    "\n",
    "If you use linear algebra to deal with a nonlinear system, it will be by finding something that is linear and is associated with the nonlinear system. Often it’s that you have a linear approximation to the nonlinear system. For instance the Jacobian of a function of several variables (see Jacobian matrix and determinant - Wikipedia ) is linear, and provides an approximation to a smoothly varying function in a neighborhood of a given initial value.\n",
    "\n",
    "Kalman filtering (see Kalman filter - Wikipedia ) is an application of this idea that is used a lot. Systems for control of nonlinear processes often use the fact that (in suitable cases) the state of a system that is close to the desired state can be approximated as a linear transformation varying in time of the initial perturbation from the desired state. Kalman filtering gives you a way to estimate parameters of a system that you can’t completely observe.\n",
    "\n",
    "There are analyses of dynamical systems that also rely on being able to approximate the effect of small perturbations linearly. The Lyapunov exponent (see Lyapunov exponent - Wikipedia ) can tell you something about the predictability of a chaotic system.\n",
    "[[src.]](https://www.quora.com/Can-linear-algebra-be-used-for-nonlinear-systems-If-yes-what-are-examples-of-this)\n",
    "\n",
    "Jacobian matrix is the matrix representing the best linear map approximation of $f$ near $(a,b)$.\n",
    "* Finite-difference Methods\n",
    "* Steffensen's Method\n",
    "* Broyden's Method\n",
    "* Generalized Secant Method\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Linear Algebra vs Functional Analysis?</strong></summary>\n",
    "\n",
    "[src.](https://math.stackexchange.com/questions/1687978/what-is-functional-analysis-in-simple-words)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Key terms\n",
    "### __Origin__\n",
    "The center of space and roots of all vectors\n",
    "\n",
    "### __Vector__\n",
    "Any pairs or combinations numbers representing its location in N-dimentional space i.e. walking from origins to the end of the arrow, as a multiple of / denominated by the basis vectors.\n",
    "\n",
    "### __Span__\n",
    "A set of all vectors that you can reach with a linear combination of some given vectors. \n",
    "\n",
    "### __Basis Vector__ of a vector space\n",
    "A set of linearly independent vectors that span the full space; Think of factorizing some numbers to its common denominator with prime numbers. If one vector is linearly dependent, that means a vector can be expressed by the linear combinations of the others. If a vector is linearly independent, that means it cannot be denominated or represented by any other given vectors in space. \n",
    "\n",
    "### __Matrix__\n",
    "A mathematical notation to represent linear transformation. Each column can be interpreted as the transformation for each basis vectors. For example, a 2x2 matrix $\\begin{bmatrix} 1&-1\\\\ 2&1 \\end{bmatrix} $ means $\\hat{i}$ is scaled by 2 units in direction of y-axis, and $\\hat{j}$ scales by -1 unit on x-axis.\n",
    "\n",
    "* __Square matrix__: Matrix with the numbers of columns and rows are equal i.e. $n \\times n$.\n",
    "* __Orthogonal matrix__: Square matrix ($\\R$) whose columns (i.e. projection of basis vectors) are orthonormal (i.e. perpendicular / 90-degree angle) to each other; meaning rows are also orthonormal. This means $A^TA=I$ and $A^{-1}=A^T$. Thus, in many application that makes sense, orthogonal matrix is prefered for ease of calculation. \n",
    "* __Non-squared (Rectangular) matrix__: Matrix with different numbers of columns and rows i.e. $n \\times m$.\n",
    "* __Null matrix__: Any matrix with all elements equal to zero.\n",
    "* __Diagonal matrix__: A square matrix with all elements except the elements along the principal diagonal ($a_{11},a_{22},a_{33}$) are equal to zero.\n",
    "* __Identity (unit) matrix__: A square, diagonal matrix that all the elements along principal diagonal are equal to one.\n",
    "* __Triangular matrix__: A square matrix with all elements above OR below the principal diagonal in a square matrix are zero.\n",
    "* __Singular matrix__: A square matrix that has no inverse is called a singular matrix. \n",
    "\n",
    "Non-squared matrix denotes a transformation between dimensions. We can interpret the number of columns as the number of basis vectors in the new space, and the rows are final coordinates or projection in each axis. For a 3x2 matrix, a 2D space is mapped to a 3D space. In other words, we are displaying a 2D space as a plane in 3D space.   \n",
    "\n",
    "<p align=\"center\"><img src=\"./img/non_squared_matrix.png\" alt=\"Non-squared Matrix\" width=\"400\"/></p>\n",
    "\n",
    "For a 2x3 matrix, a 3D space is mapped to a 2D space. In other words, we are displaying a 3D space on a 2D space. Yes, this can feel weird and messy. This concept however is related to __dot product__ in the later section.\n",
    "\n",
    "<p align=\"center\"><img src=\"./img/non_squared_matrix2.png\" alt=\"Non-squared Matrix\" width=\"400\"/></p>\n",
    "\n",
    "For both examples, the \"Column Spaces\" (more info under __Rank__) are still considered \"full rank\" since the transformation is mapping a 2D space to a 3D space without losing a dimension. \n",
    "\n",
    "### __Eigenvector and Eigenvalue__\n",
    "In a linear transformation, there could be some vectors that will shrink or expand on the same span; these are called \"eigenvectors\". The scale at which they shrink or expand are called \"eigenvalues\". Eigenvectvors describe sets of vectors that stay on the same span. On the other hands, other vectors will be transformed off its original axis. When applying this idea to a 3D space, if there is only one eigenvector (with eigenvalue of 1), this means the linear transformation results in a rotation. And, the eigenvector can be considered as axis of rotations.\n",
    "\n",
    "<p align=\"center\"><img src=\"./img/eigenvectors.png\" alt=\"Non-squared Matrix\" width=\"400\"/></p>\n",
    "\n",
    "Symbolically, $A \\overrightarrow{v} = \\lambda \\overrightarrow{v}$. This means the linear transformation $A$ would give the same results as multiplying a scalar to a vector (we can also think of lambda as a diagonal matrix or $\\lambda I$). Therefore, to find the eigenvectors, we will have to solve $(A-\\lambda I)\\overrightarrow{v}=\\overrightarrow{0}$. In other words, we have to find a matrix that transforms $\\overrightarrow{v}$ to a zero vector.\n",
    "\n",
    "Recall determinant: The only way to satisfy such condition is when $\\det(A-\\lambda I)=0$. Solve for $\\lambda$, which is the eigenvalue. Once you have the eigenvalue, solve for $\\overrightarrow{v}$, which will indicate the eigenvector(s): $(A-\\lambda I)\\overrightarrow{v}=\\overrightarrow{0}$\n",
    "\n",
    "<p align=\"center\"><img src=\"./img/eigenvectors_determinant.png\" alt=\"Non-squared Matrix\" width=\"400\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "## 3. Matrix operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Matrix Multiplication__\n",
    "Multiplication operation represents a chain of linear transformations. This is similar to when we apply matrix transformation to a vector where each columns in $M_{1}$ can be interpreted as a vector itself. Performing the matrix multiplication returns a matrix. Operations are read from right to left: $M_{2}\\cdot M_{1}$ means applying $M_{1}$ then $M_{2}$. This convention comes from reading function notations: $f(g(x))$ where $g(x)$ is computed first. While the order matters here, the matrix multiplication operation is associative meaning $(AB)C = A(BC)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For matrix operations in numpy, np.matmul and np.dot behave the same way as matrix multiplication.\n",
      " [[4 1]\n",
      " [2 2]] \n",
      "\n",
      " [[4 1]\n",
      " [2 2]] \n",
      "\n",
      " [[4 1]\n",
      " [2 2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1, 0],[0, 1]])\n",
    "b = np.array([[4, 1],[2, 2]])\n",
    "print(\"For matrix operations in numpy, np.matmul and np.dot behave the same way as matrix multiplication.\\n\",\n",
    "      np.matmul(a, b), '\\n\\n',np.dot(a, b), '\\n\\n',a @ b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Determinant__\n",
    "Determinant measures how much a linear transformation scales the space i.e. how many times larger or smaller. If determinant is zero, that means the transformation removes a dimension from a space; for example, all vectors in a 2D plane merges to a 1D line, or a 3D space is squished into a space with zero volume such as 2D plane, a line or a single point. In other words, the vectors become linearly dependent after transformation. A negative determinant represent a concept of orientation i.e. flipping the space or inverting the orientation of space.  \n",
    "\n",
    "<p align=\"center\"><img src=\"./img/negative_determinant.png\" alt=\"Negative Determinant\" width=\"400\"/></p>\n",
    "\n",
    "\n",
    "* #### __Rank__\n",
    "  Denotes the number of dimensions in the output of a transformation. If the output of a transformation is one dimensional i.e. a line, we call that Rank 1. Rank 2 for a plane. Rank 3 for 3D space, and so on. If a transformation results in the same number of dimensions (also called \"**column space**\" since the number of columns denotes the dimensions), it is considered __full rank__. \n",
    "\n",
    "* #### __Range__\n",
    "  Range is a subspace that consists of all possible linear combinations. In other words, Range is a set of all vectors that $A$ maps to in the output space: $R(A) = \\{Ax | x \\in \\R^2\\}$\n",
    "\n",
    "\n",
    "* #### __Null Space (or Kernel)__\n",
    "  Null space is a subspace of all vectors in the input space, that maps to zero in the output space: $N(A) = \\{x \\in \\R^2 | Ax=0\\}$\n",
    "  \n",
    "  When the dimension of space collapsed i.e. determinant is zero, there is a line or space full of vectors that are transformed to land on the origin. These sets of vectors that lands on origin is called a \"null space\" or \"Kernel\" of the matrix. In other words, they are vectors that became null: $Ax=\\begin{bmatrix} 0\\\\ 0 \\end{bmatrix}$.  \n",
    "\n",
    "  <img src=\"./img/null_space.png\" alt=\"Null Space or Kernel\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Inverse Transformation__\n",
    "Matrix multiplication gives a linear system of equation a new geometrical meaning. Below, we can see that a vector of variables $\\overrightarrow{x}$. can be thought of as being transformed by the coefficients, resulting in the constants or resultant vector $\\overrightarrow{v}$. We can solve this system of equations if we transform the vector $\\overrightarrow{v}$ in reverse. This operation corresponds to a different multiplication operation, commonly called \"Inverse Transformation\" denoted as $A^{-1}$.\n",
    "\n",
    "<p align=\"center\"><img src=\"./img/linear_system_of_equations.png\" alt=\"Negative Determinant\" width=\"400\"/></p>\n",
    "\n",
    "The definition of $A^{-1}$ is any transformation where $A^{-1}A = I$ where $I$ is an identity matrix. Geometrically, this means applying $A$ then $A^{-1}$ will bring you back to the same place. Therefore, we can find $\\overrightarrow{x} = A^{-1}\\overrightarrow{v}$. As long as transformation $A$ does not shrink the space into a lower dimension (i.e. as long as $\\det(A)\\neq0$), we can find a solution to $A^{-1}A\\overrightarrow{x} = A^{-1}\\overrightarrow{v}$.\n",
    "\n",
    "Singular matrix: If $\\det(A)=0$, the space shrinks into zero and the matrix is no inverse. Geometrically, since the transformation reduces the dimension of the space, many vectors are transformed into the same vectors. Therefore, it's unknown how these vectors can be inversed or mapped back to the original space.\n",
    "\n",
    "<p align=\"center\"><img src=\"./img/linear_system_of_equations_zero_det.png\" alt=\"Negative Determinant\" width=\"400\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Change of Basis__\n",
    "How to express or translate a vector from one coordinate system in the other? This is a situation where we're looking at the same space, but the 2 coordinate systems are defined by different sets of basis vectors.  \n",
    "\n",
    "<p align=\"center\"><img src=\"./img/change_of_basis.png\" alt=\"Negative Determinant\" width=\"400\"/></p>\n",
    "\n",
    "$A^{-1}MA \\overrightarrow{v}$ suggests a mathematical sort of empathy, and can be read as:\n",
    "  1. $\\overrightarrow{v}$ is a vector in coordinate system **s** being first converted to coordinate system **t** using transformation $A$, \n",
    "  2. The transformation $M$ which is in coordinate system **t** is applied, \n",
    "  3. The result is then converted back into coordinate system **s** using the inverse transformation of $A$.\n",
    "\n",
    "<p align=\"center\"><img src=\"./img/change_of_basis2.png\" alt=\"Negative Determinant\" width=\"400\"/></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Matrix Transpose__\n",
    "Procedurally, you can transpose a matrix by rotating the matrix elements along the diagonal of the matrix. In other word, the diagonal ($x_{11}, x_{22}, x_{33}$) stays in fixed position while other elements mirror along the diagonal. The key properties of a transpose: \n",
    " * $(A+B)^T=A^T+B^T$ \n",
    " * $(AB)^T=B^TA^T$\n",
    " * $(A^T)^T=A^{T^T}=A$\n",
    " * $(A^{-1})^T=(A^T)^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Orthonormal transformation__ \n",
    "Recall that Orthogonal matrix is a matrix where all column vectors are perpendicular to one another -- aka. independent or orthonormal -- thus row vectors are also orthonormal to one another. Orthogonal matrix is any $n\\times n$ matrix if $A^TA=I$. In particular, $A$ must be (1) an invertible matrix $AB=BA=I$, and (2) $A^T=A^{-1}$. This last property makes orthogonal matrix surprisingly easy to work with since it is trivial to calculate the inverse matrix which is often computationally complex. \n",
    "\n",
    "Orthornormal transformation refers to the linear transformation that orthogonal matrices represent. In addition to being perpendicular to each other, orthornormal transformation preserves lengths of vectors and angles between vectors. In a real-number space $A:\\mathbb{R} \\rightarrow \\mathbb{R}$, an orthorgonal transformation does not stretch the space: $\\det(A)=\\pm1$. This property makes orthornormal transformation preserves the dot product, meaning that for any two vectors, their dot product before the transformation is equal to the dot product of their transformed counterparts after the transformation. Geometrically, it either refers to a rigid rotation or an improper rotation (a rotation followed by a flip).\n",
    "\n",
    "A matrix that preserves the dot product is considered special because it essentially maintains the geometric relationships between vectors, like their lengths and angles, by ensuring that the inner product (dot product) of any two vectors remains unchanged after the transformation represented by the matrix; this property is crucial in applications like rotations, reflections, and transformations in Euclidean space where preserving relative orientations and distances is important. Orthogonal matrices are used in various applications like computer graphics (rotations), physics (coordinate transformations), and data analysis (principal component analysis) where preserving geometric relationships is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 📘 \n",
    "> ### Gram-Schmidt process to construct Orthonormal basis\n",
    "> Gram-Schmidt process is a method of finding a set of vectors that are perpendicular to each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Singular Value Decomposition (SVD)__\n",
    "You can breakdown any transformation into 3-step sequence: $R_2 \\Sigma R_1$ or more conventionally $U \\Sigma V^T$\n",
    "   1. Rotation/Reflection ($Rotation_1$ conventionally denoted as $V^T$)\n",
    "   2. Scaling of space at the Axes (denoted as $\\Sigma$) \n",
    "      * Note that scaling is a diagonal matrix. Thus, the transpose of $\\Sigma$ is $\\Sigma$.\n",
    "   3. Rotation/Reflection ($Rotation_2$ conventionally denoted as $U$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vector operations\n",
    "### __Dot Product__\n",
    "> __Duality__ is a situation where you have a natural but suprising correspondence between 2 mathematical concepts. In this case,  In other word, the dual of a vector is the linear transformation that it encodes e.g. $\\begin{bmatrix}x_{1} \\\\x_{2} \\\\ \\vdots \\end{bmatrix} \\Leftrightarrow \\begin{bmatrix}x_{1} & x_{2} & \\dots \\end{bmatrix}$. On the other hand, a dual of a transformation from some space to one dimension is the vector in that space: $\\begin{bmatrix}x_{1} & x_{2} & \\dots \\end{bmatrix}\\Leftrightarrow \\begin{bmatrix}x_{1} \\\\x_{2} \\\\ \\vdots \\end{bmatrix}$\n",
    "\n",
    "Dot product is a *vector* operation which results in a scalar value. Dot product tells us the magnitude of transformation of vector $\\bold{a}$ in the direction of $\\bold{b}$. One application in Physics is work done: $W = \\overrightarrow{F} \\cdot \\Delta \\overrightarrow{d} = |\\overrightarrow{F}||\\Delta \\overrightarrow{d}|\\cos{\\Theta}$. \n",
    "\n",
    "<p align=\"center\"><img src=\"./img/duality.png\" alt=\"Work done\" height=\"200\"/> <span/> <img src=\"./img/eq_workdone.jpg\" alt=\"Work done\" height=\"200\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Deep Dive 1) Linear transformations do not necessarily preserve the dot product of 2 vectors\n",
    "Dot product of 2 vectors can be represented by transpose: $v \\cdot x=v^Tx$.\n",
    "\n",
    "Consider the statement: Linear transformation do not always preserve the result of a dot product. Say we have 2 vectors $x$ and $v$, dot product of $x \\cdot v$ is the magnitude or projection of $x$ in the $v$ direction. In the presence of linear transformation of the space, dot product of the vectors may not be preserved. In other word, $\\neg\\square(x \\cdot v \\neq \\bar{x} \\cdot \\bar{v})$; read as \"not necessarily equal\". \n",
    "* Before transformation: $v \\cdot x=v^Tx$\n",
    "* After transformation: $\\bar{v} \\cdot \\bar{x}=\\bar{v}^T\\bar{x}=(Av)^T(Ax)=v^T(A^TA)x$\n",
    "* Linear transformation will preserve dot product only if $A^TA=I$. In other words, $v \\cdot x=\\bar{v} \\cdot \\bar{x}$ only if $A^T=A^{-1}$. This type of linear transformation is also called an __orthogonal matrix__.\n",
    "\n",
    "An \"invariant dot product\" refers to a dot product that remains unchanged (invariant) under a specific transformation, like rotation in Euclidean geometry, meaning the value of the dot product between two vectors will be the same regardless of how the coordinate system is rotated; essentially, it signifies that the geometric relationship between the vectors is preserved under the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Deep Dive 2) What if we are given a transformation $A$ for $x$, and need to find another transformation $M$ that preserves the dot product of any arbitrary $\\bar{v} \\cdot \\bar{x}$ in the space? \n",
    "\n",
    "Using the same idea, we can use transpose to find the linear transformation that preserve the dot product. \n",
    "* Say we are given a transformation $A$: $\\bar{x} = A\\cdot x$\n",
    "* Let an unknown $M$ matrix applies to $v$: $\\bar{v} = M\\cdot v$\n",
    "* Substituting $v \\cdot x=\\bar{v} \\cdot \\bar{x}$, we have $v^Tx = (Mv)^T(Ax) \\longrightarrow M^TA=I \\longrightarrow M=(A^{-1})^T$.\n",
    "\n",
    "If $A$ happens to be an orthorgonal matrix i.e. $A^T=A^{-1}$, that means the transformation using transposed matrix ($A^T$) will undo ($A^{-1}$) the original linear transformation: $M=(A^{-1})^T=A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Deep Dive 3) Let's explore further with Singular Value Decomposition (SVD) \n",
    "Recall SVD: We can express any linear transformation as $A=R_2 \\Sigma R_1$. We can apply SVD to the forward transformation rule we have above:\n",
    "* $\\bar{x}=Ax=(R_2 \\Sigma R_1)x$\n",
    "* $\\bar{v}=(A^{-1})^Tv=(R_2 \\Sigma R_1)^{-1^T}v=(R_1^{-1} \\Sigma^{-1} R_2^{-1})^{T}v$ \n",
    "  \n",
    "Recall the property: \n",
    "* $\\Sigma$ is a diagonal matrix with the property $\\Sigma^{-1^T}=\\Sigma^{-1}$\n",
    "* $R$ is an orthorgonal matrix with the property $R^{-1^T}=R$.\n",
    "\n",
    "Therefore, to continue:\n",
    "* $\\bar{v}=(R_1^{-1} \\Sigma^{-1} R_2^{-1})^{T}v=(R_2^{-1^T} \\Sigma^{-1^T} R_1^{-1^T})v=(R_2 \\Sigma^{-1}R_1)v$\n",
    "\n",
    "This means, if our goal is to preserve the dot product of 2 vectors, we can solve this by decompose $A=R_2 \\Sigma R_1$, then find $\\Sigma^{-1}$. In plain words, scaling $x$ using $\\Sigma$ and scaling $v$ using the inverse of $\\Sigma$ means that $\\bar{x}$ is scaled \"up\" as much as $\\bar{v}$ is scaled \"down\" and so the dot product is retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Deep Dive 4) Visual intuition\n",
    "$v \\cdot x=\\bar{v} \\cdot \\bar{x}$ \n",
    "\n",
    "Where:\n",
    "* $\\bar{x}=Ax$  \n",
    "* $\\bar{v}=A^{-1^T}v \\longrightarrow A^T\\bar{v}=v$  \n",
    "\n",
    "Therefore: $(A^T\\bar{v})\\cdot x = \\bar{v}\\cdot (Ax)$\n",
    "\n",
    "Transpose is somewhat analogous to an inverse. Particularly, an inverse of a matrix transform an arbitrary vector back to its original space. Similarly, a transposed matrix transform back from an output space to an input space. This is easy to see with a non-square matrix, $A:\\R^3 \\rightarrow \\R^2 ~~\\text{and}~~ A^T:\\R^2 \\rightarrow \\R^3$. \n",
    "\n",
    "That said, a transpose is not exactly the same as an inverse. Again with the non-square matrix, we can see that $A$ is not invertible since we are losing the information from the third dimension. However, the matrix still have a transpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./img/matrix_transpose_intuition1.png\" alt=\"Transpose analogous to inverse\" style=\"width:400px\"></p>\n",
    "<p align=\"center\"><img src=\"./img/matrix_transpose_intuition2.png\" alt=\"Transpose analogous to inverse\" style=\"width:400px\"></p>\n",
    "<p align=\"center\"><img src=\"./img/matrix_transpose_intuition3.png\" alt=\"Transpose analogous to inverse\" style=\"width:400px\"></p>\n",
    "<p align=\"center\"><img src=\"./img/matrix_transpose_intuition4.png\" alt=\"Transpose analogous to inverse\" style=\"width:400px\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Cross Product__\n",
    "Cross product is a *vector* operation which results in a vector perpendicular to both vectors. The dual of cross product is determinant. Specifically, the magnitude i.e. length of the cross-product vector is the same magnitude as determinant, and the direction of the vector indicates the \"sign\" of determinant.  \n",
    "\n",
    "<p align=\"center\"><img src=\"./img/duality2.png\" alt=\"Work done\" height=\"200\"/> <span/> <img src=\"./img/eq_crossproduct.png\" alt=\"Torque\" height=\"200\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques to compute linear system of equations\n",
    "   1. Cramer's rule: slow but can understand geometrically \n",
    "   2. Gaussian Elimination: fast"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
